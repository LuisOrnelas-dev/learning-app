# ü§ñ Configuraci√≥n de Ollama (IA Local)

## ¬øQu√© es Ollama?

Ollama es una herramienta que te permite ejecutar modelos de IA localmente en tu computadora, sin necesidad de APIs externas ni costos. Es perfecto para:

- ‚úÖ **Privacidad total** - Los datos nunca salen de tu computadora
- ‚úÖ **Sin costos** - No hay l√≠mites de uso ni facturaci√≥n
- ‚úÖ **Funciona offline** - Una vez descargado, no necesita internet
- ‚úÖ **Modelos gratuitos** - Llama, Mistral, CodeLlama, etc.

## üöÄ Instalaci√≥n R√°pida

### 1. Descargar Ollama
Ve a [https://ollama.ai](https://ollama.ai) y descarga la versi√≥n para tu sistema operativo.

### 2. Instalar y ejecutar
```bash
# En macOS/Linux, despu√©s de descargar:
chmod +x ollama
./ollama serve
```

### 3. Descargar un modelo
```bash
# Modelo recomendado para empezar:
ollama pull llama2

# Otras opciones:
ollama pull mistral    # M√°s r√°pido
ollama pull codellama  # Para c√≥digo
ollama pull llama2:7b  # Versi√≥n m√°s peque√±a
```

## üîß Configuraci√≥n en la App

1. **Ejecuta Ollama** en segundo plano
2. **Abre la app** de TechFlow Academy
3. **Haz clic en "Local AI"** en la barra superior
4. **Prueba la conexi√≥n** con el bot√≥n "Test Models"

## üìã Modelos Disponibles

| Modelo | Tama√±o | Velocidad | Uso Recomendado |
|--------|--------|-----------|-----------------|
| `llama2` | 7B | Medio | General |
| `mistral` | 7B | R√°pido | General |
| `codellama` | 7B | Medio | C√≥digo |
| `llama2:13b` | 13B | Lento | Mejor calidad |
| `llama2:7b` | 7B | R√°pido | B√°sico |

## ‚ö° Optimizaci√≥n

### Para mejor rendimiento:
```bash
# Usar GPU (si tienes NVIDIA):
ollama pull llama2:7b

# Usar CPU optimizado:
ollama pull llama2:7b-q4_0
```

### Para menos uso de memoria:
```bash
# Modelos cuantizados (m√°s peque√±os):
ollama pull llama2:7b-q4_0
ollama pull mistral:7b-q4_0
```

## üîç Soluci√≥n de Problemas

### Ollama no responde
```bash
# Verificar que est√© ejecut√°ndose:
curl http://localhost:11434/api/tags

# Reiniciar Ollama:
pkill ollama
ollama serve
```

### Error de memoria
- Usa modelos m√°s peque√±os (`llama2:7b` en lugar de `llama2:13b`)
- Cierra otras aplicaciones que usen mucha RAM
- Usa modelos cuantizados (`-q4_0`)

### Lento
- Usa modelos m√°s peque√±os
- Aseg√∫rate de tener suficiente RAM libre
- Considera usar GPU si tienes NVIDIA

## üéØ Ventajas vs OpenAI API

| Caracter√≠stica | Ollama Local | OpenAI API |
|----------------|---------------|------------|
| **Costo** | Gratis | $0.0015-0.002 por 1K tokens |
| **Privacidad** | 100% local | Datos en servidores de OpenAI |
| **Velocidad** | Depende de tu hardware | Muy r√°pida |
| **Calidad** | Buena (7B-13B) | Excelente (GPT-4) |
| **Offline** | S√≠ | No |
| **Configuraci√≥n** | Requiere instalaci√≥n | Solo API key |

## üöÄ Pr√≥ximos Pasos

1. **Instala Ollama** siguiendo los pasos arriba
2. **Descarga un modelo** (`ollama pull llama2`)
3. **Prueba la app** en modo Local AI
4. **Experimenta** con diferentes modelos

¬°Disfruta de tu IA local! üéâ 