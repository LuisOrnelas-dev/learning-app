#!/bin/bash

echo "ğŸ¤– Instalador de Ollama para TechFlow Academy"
echo "=============================================="

# Detectar sistema operativo
if [[ "$OSTYPE" == "darwin"* ]]; then
    echo "ğŸ“± Detectado: macOS"
    OS="macos"
elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
    echo "ğŸ§ Detectado: Linux"
    OS="linux"
else
    echo "âŒ Sistema operativo no soportado: $OSTYPE"
    exit 1
fi

# Verificar si Ollama ya estÃ¡ instalado
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama ya estÃ¡ instalado"
    echo "ğŸš€ Iniciando Ollama..."
    ollama serve &
    sleep 3
    
    echo "ğŸ“¥ Descargando modelo llama2..."
    ollama pull llama2
    
    echo "ğŸ‰ Â¡ConfiguraciÃ³n completada!"
    echo "ğŸ’¡ Ahora puedes usar la app en modo 'Local AI'"
    exit 0
fi

# Instalar Ollama
echo "ğŸ“¥ Descargando Ollama..."

if [[ "$OS" == "macos" ]]; then
    # macOS
    curl -fsSL https://ollama.ai/install.sh | sh
elif [[ "$OS" == "linux" ]]; then
    # Linux
    curl -fsSL https://ollama.ai/install.sh | sh
fi

# Verificar instalaciÃ³n
if ! command -v ollama &> /dev/null; then
    echo "âŒ Error: Ollama no se instalÃ³ correctamente"
    echo "ğŸ’¡ Intenta instalarlo manualmente desde https://ollama.ai"
    exit 1
fi

echo "âœ… Ollama instalado correctamente"

# Iniciar Ollama
echo "ğŸš€ Iniciando Ollama..."
ollama serve &
sleep 5

# Descargar modelo
echo "ğŸ“¥ Descargando modelo llama2 (esto puede tomar varios minutos)..."
ollama pull llama2

# Verificar que todo funcione
echo "ğŸ” Verificando instalaciÃ³n..."
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama estÃ¡ funcionando correctamente"
    echo "ğŸ‰ Â¡ConfiguraciÃ³n completada!"
    echo ""
    echo "ğŸ’¡ PrÃ³ximos pasos:"
    echo "1. Abre la app de TechFlow Academy"
    echo "2. Haz clic en 'Local AI' en la barra superior"
    echo "3. Prueba la conexiÃ³n con 'Test Models'"
    echo "4. Â¡Disfruta de tu IA local!"
else
    echo "âŒ Error: Ollama no responde en http://localhost:11434"
    echo "ğŸ’¡ Intenta reiniciar Ollama manualmente:"
    echo "   ollama serve"
fi 